The following README file will cover installation, usage, and a brief overview of the code sections.

TITLE - APPAREL CLASSIFICATION

#PROJECT DESCRIPTION
This Apparel recognition project of ours involves building a dynamic classification model from the images acquired from a fashion product images dataset from Kaggle.
We then constructed an end-to-end pipeline for multi-class image classification leveraging big data tools such as Apache Spark and Hadoop Distributed File System (HDFS) combined with transfer learning techniques like RESNET50.

#SYSTEM REQUIREMENTS
Google Collab Pro Plus subscription to compute instances.
Internet connection to access Hadoop and download data from Kaggle.
Hadoop HDFS â€“ File Storage

#INSTALLATION
Set up Hadoop:
Set up Hadoop by installing, configuring, and starting services, ensuring proper settings for distributed storage and processing of large datasets.

Install Required Libraries:
Install the necessary Python libraries using pip. It's recommended to use a virtual environment.
!pip install numpy pandas pyspark tensorflow keras opencv-python-headless pillow matplotlib seaborn tqdm scikit-learn pickle

Download Data:
Use Kaggle API to download the necessary apparel classification image dataset. Ensure you have the Kaggle API key configured.
!kaggle datasets download -d enashed/fashion-dataset
https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-dataset
!unzip fashion-dataset.zip -d /content/fashion-dataset

Usage:
Data Collection:
* Setup Hadoop Cluster: Install and configure Hadoop on servers for cluster setup.
* Data Ingestion: Use Hadoop command-line tools to ingest data into HDFS.
* hdfs dfs -copyFromLocal <local_src> <hdfs_dest>
* Data Formats: Choose appropriate data formats 
* Data Partitioning and Compression: Optimize storage and query performance through data partitioning and compression techniques.
* Data Access Control: Implement access control mechanisms for data security.
Execute the spark program to create a baseline dataset with labelled folder from the raw images dump 

Data Cleaning and Preprocessing:
* Convert the data into a Spark DataFrame.
* Resizing the derived images based on size and aspect ratio.

3. Model Building:
* Train a transfer learning model using TensorFlow/Keras.
* Split the data into training, validation, and test sets.
* Perform feature extraction using sequential learning.

5. Visualization and Results:
* Generate predictions using the trained model.
* Display images with predicted and actual labels.
* Plot learning curves, Confusion of predicted probabilities.

Running the Project:
* Open the project in Google Collab.
* Run each cell in the provided notebook sequentially to perform data collection, preprocessing, model training, and evaluation.

Contributing:
* Contributions to the project are welcome. Please fork the repository, make changes, and submit a pull request.

This README provides a detailed guide to setting up and running the project. Modify paths and environment settings as necessary to match your setup.
